# Taken from: https://picluster.ricsanfre.com/docs/prometheus/

prometheusOperator:
  # Resources for the operator
  resources:
    requests:
      memory: 192Mi
      cpu: 75m
    limits:
      memory: 384Mi
      cpu: 300m
  # Relabeling job name for operator metrics
  serviceMonitor:
    relabelings:
      # Replace job value
      - sourceLabels:
          - __address__
        action: replace
        targetLabel: job
        replacement: prometheus-operator
  # Disable creation of kubelet service
  kubeletService:
    enabled: false
alertmanager:
  alertmanagerSpec:
    # Subpath /alertmanager configuration
    externalUrl: http://monitoring.${NETWORK_HOSTNAME_SUFFIX}/alertmanager/
    routePrefix: /
    # Resources configuration
    resources:
      requests:
        memory: 128Mi
        cpu: 50m
      limits:
        memory: 256Mi
        cpu: 250m
    # PVC configuration
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
  # ServiceMonitor job relabel
  serviceMonitor:
    relabelings:
      # Replace job value
      - sourceLabels:
          - __address__
        action: replace
        targetLabel: job
        replacement: alertmanager
prometheus:
  prometheusSpec:
    containers:
      - name: prometheus
        startupProbe:
          failureThreshold: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 5
          timeoutSeconds: 10
          periodSeconds: 5
    # Subpath /prometheus configuration
    externalUrl: http://monitoring.${NETWORK_HOSTNAME_SUFFIX}/prometheus/
    routePrefix: /
    # Data retention configuration
    retention: 21d
    retentionSize: 12GB
    walCompression: true
    # Resources request and limits
    resources:
      requests:
        memory: 768Mi
        cpu: 300m
      limits:
        memory: 2Gi
        cpu: 1500m
    # PVC configuration
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 15Gi
    # Global scrape configuration
    scrapeInterval: 60s
    scrapeTimeout: 15s
    evaluationInterval: 60s
    additionalScrapeConfigs:
      - job_name: "uptime"
        scrape_interval: 300s
        scrape_timeout: 15s
        scheme: http
        metrics_path: "/metrics"
        static_configs:
          - targets: ["uptime.${NETWORK_HOSTNAME_SUFFIX}"]
        basic_auth:
          username: ${UPTIME_USERNAME}
          password: ${UPTIME_PASSWORD}
  # ServiceMonitor job relabel
  serviceMonitor:
    relabelings:
      # Replace job value
      - sourceLabels:
          - __address__
        action: replace
        targetLabel: job
        replacement: prometheus
grafana:
  # Configuring /grafana subpath
  grafana.ini:
    server:
      domain: monitoring.${NETWORK_HOSTNAME_SUFFIX}
      root_url: "%(protocol)s://%(domain)s/grafana/"
      serve_from_sub_path: true
    database:
      # Use PostgreSQL for better concurrency and reliability
      type: postgres
      host: 10.43.100.50:5432
      name: grafana
      # IMPORTANT: Do not set 'user' and 'password' here directly
      # The Helm chart validation (assertNoLeakedSecrets) prevents hardcoded sensitive values
      # Instead, we use environment variables GF_DATABASE_USER and GF_DATABASE_PASSWORD
      # which Grafana automatically picks up for database configuration
      ssl_mode: disable
      # PostgreSQL can handle more connections than SQLite
      max_open_conn: 10
      max_idle_conn: 2
      conn_max_lifetime: 14400
  # Inject database credentials from secret
  # Grafana automatically uses GF_DATABASE_USER and GF_DATABASE_PASSWORD environment variables
  # This approach satisfies the Helm chart's assertNoLeakedSecrets validation
  envFromSecret: monitoring-secret
  # Admin user password
  adminPassword: ${GRAFANA_PASSWORD}
  # Set timezone to be browser
  defaultDashboardsTimezone: browser
  # Use Recreate strategy to avoid multi-attach issues with ReadWriteOnce PVC
  deploymentStrategy:
    type: Recreate
  # No extra plugins; avoids unsupported Angular panels on Grafana 11+
  # Resources configuration - increased for better performance
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      memory: 768Mi
      cpu: 500m
  # Improved liveness and readiness probes for Raspberry Pi
  livenessProbe:
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 120
    timeoutSeconds: 30
    periodSeconds: 30
    failureThreshold: 5
  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 30
    timeoutSeconds: 10
    periodSeconds: 10
    failureThreshold: 10
  # Persistence configuration
  persistence:
    enabled: true
    type: pvc
    size: 5Gi
    accessModes:
      - ReadWriteOnce
    storageClassName: longhorn
  # Pod scheduling - avoid co-locating with Prometheus
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: prometheus
            topologyKey: kubernetes.io/hostname
  # ServiceMonitor label and job relabel
  serviceMonitor:
    labels:
      release: monitoring
    relabelings:
      # Replace job value
      - sourceLabels:
          - __address__
        action: replace
        targetLabel: job
        replacement: grafana
  sidecar:
    dashboards:
      enabled: true
      labelValue: "true"
      # Reduce sidecar resource usage and polling frequency
      resource: both  # Watch both ConfigMaps and Secrets
      searchNamespace: ALL
      # Increase watch interval to reduce database load
      watchMethod: WATCH
      # Resource limits for sidecar
      resources:
        requests:
          memory: 64Mi
          cpu: 25m
        limits:
          memory: 128Mi
          cpu: 100m
# Disabling monitoring of K8s services.
# Monitoring of K3S components will be configured out of monitoring
kubelet:
  enabled: false
kubeApiServer:
  enabled: false
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
kubeProxy:
  enabled: false
kubeEtcd:
  enabled: false
# Disable K8S Prometheus Rules
# Rules for K3S components will be configured out of monitoring
defaultRules:
  create: true
  rules:
    etcd: false
    k8s: false
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: false
    kubeProxy: false
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: true
    kubeScheduler: false
# Node Exporter configuration
nodeExporter:
  enabled: true
  resources:
    requests:
      memory: 48Mi
      cpu: 30m
    limits:
      memory: 96Mi
      cpu: 100m

# Kube State Metrics configuration
kube-state-metrics:
  resources:
    requests:
      memory: 96Mi
      cpu: 75m
    limits:
      memory: 192Mi
      cpu: 250m

crds:
  upgradeJob:
    enabled: true
