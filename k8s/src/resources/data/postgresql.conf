# PostgreSQL performance tuning for Raspberry Pi + Longhorn storage
# Optimized for: 512Mi-1Gi memory, network-attached SSD storage, multiple small databases

# CONNECTION SETTINGS
# Listen on all interfaces to accept connections from within the cluster
listen_addresses = '*'

# Limit connections to prevent resource exhaustion on Raspberry Pi
max_connections = 50

# MEMORY SETTINGS
# shared_buffers: 25% of allocated memory (768Mi request = ~192MB)
# Conservative for Raspberry Pi to leave room for OS and other processes
shared_buffers = 192MB

# huge_pages: Use Linux huge pages for better memory management
# 'try' attempts to use huge pages but falls back gracefully
huge_pages = try

# effective_cache_size: Estimate of memory available for disk caching
# Set to ~66% of pod limit (1536Mi = ~1GB) to help query planner
effective_cache_size = 1GB

# work_mem: Memory for sorting/hashing per operation
# Keep low due to max_connections; total can be work_mem * max_connections
work_mem = 4MB

# maintenance_work_mem: Memory for VACUUM, CREATE INDEX, etc.
# Higher value speeds up maintenance operations (increased with more RAM)
maintenance_work_mem = 128MB

# WAL (Write-Ahead Log) SETTINGS
# Optimized for Longhorn network storage to reduce fsync overhead

# wal_buffers: Amount of shared memory for WAL data
# Auto-tuned based on shared_buffers (typically ~3% = 4MB)
wal_buffers = 4MB

# min_wal_size: Minimum size to keep for future checkpoints
min_wal_size = 80MB

# max_wal_size: Maximum WAL size between checkpoints
# Larger value = less frequent checkpoints = better performance on network storage
max_wal_size = 1GB

# synchronous_commit: Trade-off between performance and durability
# 'on' = full durability (default, safer for production)
# For Longhorn (network storage), keep 'on' to ensure data safety
synchronous_commit = on

# wal_compression: Compress WAL data to reduce I/O over network
wal_compression = on

# CHECKPOINT SETTINGS
# Reduce checkpoint frequency to minimize I/O spikes on Longhorn

# checkpoint_timeout: Max time between checkpoints (default 5min)
# Increase to 15min to reduce I/O pressure
checkpoint_timeout = 15min

# checkpoint_completion_target: Spread checkpoint I/O over time
# 0.9 = spread over 90% of checkpoint interval (gentler on storage)
checkpoint_completion_target = 0.9

# QUERY PLANNER SETTINGS
# Optimized for SSD-backed Longhorn storage

# random_page_cost: Relative cost of random vs sequential I/O
# 4.0 = default for spinning disks
# 1.1 = SSDs (Longhorn typically uses SSD backing storage)
random_page_cost = 1.1

# effective_io_concurrency: Number of concurrent I/O operations
# Increase for SSD-backed storage (default is 1)
effective_io_concurrency = 200

# BACKGROUND WRITER SETTINGS
# Tuned to reduce checkpoint I/O spikes

bgwriter_delay = 200ms
bgwriter_lru_maxpages = 100
bgwriter_lru_multiplier = 2.0

# PARALLEL QUERY SETTINGS
# Enable parallel queries for better multi-core utilization on Raspberry Pi

# max_parallel_workers_per_gather: Workers per parallel query
# Set to 2 for Raspberry Pi (4 cores, but leave headroom)
max_parallel_workers_per_gather = 2

# max_parallel_workers: Total parallel workers across all queries
max_parallel_workers = 4

# max_worker_processes: Background worker processes
max_worker_processes = 6

# parallel_tuple_cost: Cost estimate for parallel query processing
# Lower value encourages parallelism (default 0.1)
parallel_tuple_cost = 0.01

# parallel_setup_cost: Startup cost for parallel workers
# Lower value encourages parallelism (default 1000)
parallel_setup_cost = 100

# min_parallel_table_scan_size: Minimum table size for parallel scan
# Lower threshold for smaller databases (default 8MB)
min_parallel_table_scan_size = 4MB

# VACUUM SETTINGS
# More aggressive autovacuum for small databases

autovacuum = on
autovacuum_max_workers = 2
autovacuum_naptime = 30s

# Autovacuum cost-based delay to prevent I/O spikes
autovacuum_vacuum_cost_delay = 10ms
autovacuum_vacuum_cost_limit = 400

# LOGGING SETTINGS
# Enable slow query logging for performance monitoring
# Reduce noise from health check probes

log_min_duration_statement = 1000  # Log queries taking >1s
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on

# Log connections/disconnections but exclude health check noise
# Note: PostgreSQL doesn't have built-in filtering, so we keep these off
# to reduce log volume from frequent pg_isready probes (every 10-30s)
log_connections = off
log_disconnections = off
log_lock_waits = on

# LOCALIZATION
# Match container defaults
lc_messages = 'en_US.utf8'
lc_monetary = 'en_US.utf8'
lc_numeric = 'en_US.utf8'
lc_time = 'en_US.utf8'

# TIMEZONE
timezone = 'UTC'

# CONNECTION POOLING OPTIMIZATION
# Reduce connection overhead for frequent short queries

# tcp_keepalives: Keep idle connections alive to reduce reconnection overhead
tcp_keepalives_idle = 60        # Seconds before sending keepalive (default 7200)
tcp_keepalives_interval = 10    # Seconds between keepalives (default 75)
tcp_keepalives_count = 5        # Max keepalive probes (default 9)

# RESOURCE CLEANUP
# More aggressive cleanup for limited resources

shared_preload_libraries = 'pg_stat_statements'

# pg_stat_statements configuration
pg_stat_statements.max = 1000           # Track top 1000 queries
pg_stat_statements.track = all          # Track all queries including nested
pg_stat_statements.track_utility = on   # Track utility commands

# Statistics collection for query optimization
track_activities = on
track_counts = on
track_io_timing = on
track_functions = all

# Statement timeout to prevent runaway queries
statement_timeout = 300000  # 5 minutes max per query

# Idle transaction timeout to prevent connection leaks
idle_in_transaction_session_timeout = 600000  # 10 minutes max idle
